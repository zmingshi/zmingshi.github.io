<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zmingshi@博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-06-02T04:26:48.817Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>logistic_regression</title>
    <link href="http://yoursite.com/2017/06/01/logistic-regression/"/>
    <id>http://yoursite.com/2017/06/01/logistic-regression/</id>
    <published>2017-06-01T10:26:30.000Z</published>
    <updated>2017-06-02T04:26:48.817Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>$$h_{\vec w}(\mathbf x) = \sum_{i=0}^n w_ix_i = w^T \mathbf x$$</p>
<h3 id="1-似然函数"><a href="#1-似然函数" class="headerlink" title="1. 似然函数"></a>1. 似然函数</h3><p>$$y^{(i)} = w^Tx^{(i)} + \epsilon^{(i)}$$</p>
<ul>
<li>其中$x^{(i)}$ 为第$i$个样本, 误差$\epsilon^{(i)}$ 服从均值为0，方差为$\sigma^2$ 的高斯分布(中心极限定理), 共$m$ 个样本.<br>$$p\left(\epsilon^{(i)}\right) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(- \frac{\left(\epsilon^{(i)}\right)^2}{2\sigma^2} \right)$$</li>
</ul>
<p>$$p\left(y^{(i)}|x^{(i)};w \right) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(- \frac{\left(y^{(i)} - w^Tx^{(i)}\right)^2}{2\sigma^2} \right)$$</p>
<p>$$\begin{align}<br>L(w) &amp;= \prod_{i=1}^m p\left(y^{(i)}|x^{(i)};w \right) \\\\<br>          &amp;= \prod_{i=1}^m \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(- \frac{\left(y^{(i)} - w^Tx^{(i)}\right)^2}{2\sigma^2} \right)<br>\end{align}$$</p>
<ul>
<li><p>上述似然函数取对数得到如下结果：<br>$$<br>\begin{align}<br>l(w) &amp;= \log L(w) \\\\<br>   &amp;= \prod_{i=1}^m\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(y^{(i)} - w^Tx^{(i)}\right)^2}{2\sigma^2} \right) \\\\<br>   &amp;= \sum_{i=1}^m\log \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(y^{(i)} - w^Tx^{(i)}\right)^2}{2\sigma^2} \right) \\\\<br>   &amp;= m\log \frac{1}{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2}\sum_{i=1}^m\left(y^{(i)} - w^Tx^{(i)}\right)^2<br>\end{align}<br>$$</p>
</li>
<li><p>最小二乘损失函数如下:<br>$$\begin{align}<br>J(w) &amp;= \frac{1}{2}\sum_{i=1}^m\left(w^Tx^{(i)} - y^{(i)}\right)^2 \\\\<br>&amp;= \frac{1}{2}(Xw - y)^T(Xw-y)<br>\end{align}$$</p>
</li>
<li><p>求解梯度得到驻点，从而得到解析结果：<br>$$w = \left(X^TX\right)^{-1}X^Ty$$</p>
</li>
<li>防止$X^TX$ 不可逆或者说过拟合, 加入$\lambda$ 扰动：<br>$$w = \left(X^TX + \lambda I\right)^{-1}X^Ty$$</li>
<li>由于$X^TX$为半正定矩阵，则对于任意$\lambda &gt; 0$， 则$\left(X^TX + \lambda I\right)$ 一定正定，从而保证可逆： 对于任意非零向量$u$,  $u^T\left(X^TX + \lambda I\right)u = u^TX^TXu + \lambda u^Tu$ &gt; 0</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h2&gt;&lt;p&gt;$$h_{\vec w}(\mathbf x) = \sum_{i=0}^n w_ix_i = w^T \mathbf x$
    
    </summary>
    
    
  </entry>
  
</feed>
